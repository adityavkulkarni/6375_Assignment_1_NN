Activation,Gradient,Optimizer,Learning Rate,Epochs,Loss,Training Accuracy, Test Accuracy, Time
sigmoid,batch,adagrad,0.001,50,0.0446182264236794,0.9027237354085604,0.9427083333333334,20.007093
sigmoid,batch,momentum,0.005,100,0.0260236810163514,0.9455252918287936,0.9622395833333334,32.330628
sigmoid,batch,none,0.05,100,0.0337930196972586,0.926070038910506,0.9518229166666666,30.258089
tanh,batch,adagrad,0.0001,100,0.0116343433380671,0.9688715953307392,0.9817708333333334,40.487825
tanh,batch,momentum,0.005,100,0.0193437589836754,0.9610894941634242,0.9791666666666666,32.728432
tanh,batch,none,0.05,100,0.0326880188160667,0.933852140077821,0.96875,29.465989
relu,batch,adagrad,5e-05,50,0.0417189878651427,0.9105058365758756,0.9635416666666666,19.215655
relu,batch,momentum,0.008,100,0.0480208318530721,0.9027237354085604,0.9518229166666666,31.604579
relu,batch,none,0.05,100,0.03734529549030238,0.9221789883268483,0.9791666666666666,29.019841
